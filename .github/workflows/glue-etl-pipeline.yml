name: Terraform + ETL Automation

on:
  push:
    branches:
      - Gyan



jobs:
  infra-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        working-directory: ./infra
        run: terraform init


      - name: Terraform Validate
        working-directory: ./infra
        run: terraform validate

      - name: Terraform Apply
        working-directory: ./infra
        run: terraform apply -auto-approve

      - name: Get Terraform outputs
        id: tf_outputs
        working-directory: ./infra
        run: |
          ETL_JOB_NAME=$(terraform output -raw glue_job_name)
          CRAWLER_NAME=$(terraform output -raw crawler_name)
          echo "ETL_JOB_NAME=$ETL_JOB_NAME" >> $GITHUB_ENV
          echo "CRAWLER_NAME=$CRAWLER_NAME" >> $GITHUB_ENV

  run-etl:
    needs: infra-deploy
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Upload ETL Script to S3 with new folder
        run: |
          BUCKET_NAME="third-glue-bkt-grp-three-nyc"
          PREFIX="scripts/run"
          COUNT=1
          while aws s3 ls s3://$BUCKET_NAME/${PREFIX}${COUNT}/ > /dev/null 2>&1; do
            COUNT=$((COUNT+1))
          done
          FOLDER="${PREFIX}${COUNT}"
          aws s3 cp ${{ env.SCRIPT_PATH }} s3://$BUCKET_NAME/$FOLDER/etl-glue-script.py
          echo "Script uploaded to s3://$BUCKET_NAME/$FOLDER/"

      - name: Start Glue ETL Job
        id: start_etl
        run: |
          JOB_RUN_ID=$(aws glue start-job-run --job-name ${{ env.ETL_JOB_NAME }} --query 'JobRunId' --output text)
          echo "ETL JobRunId: $JOB_RUN_ID"
          echo "job_run_id=$JOB_RUN_ID" >> $GITHUB_OUTPUT

      - name: Wait for ETL Job Completion
        run: |
          JOB_RUN_ID=${{ steps.start_etl.outputs.job_run_id }}
          while true; do
            STATUS=$(aws glue get-job-run --job-name ${{ env.ETL_JOB_NAME }} --run-id "$JOB_RUN_ID" --query 'JobRun.JobRunState' --output text)
            echo "Current ETL job status: $STATUS"
            if [ "$STATUS" == "SUCCEEDED" ]; then
              echo "ETL Job completed successfully."
              break
            elif [ "$STATUS" == "FAILED" ] || [ "$STATUS" == "STOPPED" ]; then
              echo "ETL Job failed or stopped."
              exit 1
            fi
            sleep 30
          done

      - name: Get Terraform outputs
        working-directory: ./infra
        run: |
          CRAWLER_NAME=$(terraform output -raw crawler_name)
          echo "CRAWLER_NAME=$CRAWLER_NAME" >> $GITHUB_ENV




      - name: Wait for Crawler to Finish
        run: |
          CRAWLER_STATUS="RUNNING"
          while [ "$CRAWLER_STATUS" == "RUNNING" ]; do
            CRAWLER_STATUS=$(aws glue get-crawler --name $CRAWLER_NAME --query 'Crawler.State' --output text)
            echo "Crawler status: $CRAWLER_STATUS"
            sleep 15
          done
#
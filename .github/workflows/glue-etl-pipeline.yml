name: Terraform + ETL Automation

on:
  push:
    branches:
      - Gyan

env:
  AWS_REGION: us-east-1
  SCRIPT_PATH: etl/etl-glue-script.py
  S3_BUCKET: third-glue-bkt-grp-three-nyc
  ETL_JOB_NAME: glue-etl-job
  CRAWLER_NAME: my-etl-crawler


jobs:
  infra-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        working-directory: ./infra
        run: terraform init


      - name: Terraform Validate
        working-directory: ./infra
        run: terraform validate

      - name: Terraform Apply
        working-directory: ./infra
        run: |
          terraform apply -auto-approve \
            -var "region=us-east-1" \
            -var "etl_bucket_name=third-glue-bkt-grp-three-nyc" \
            -var "glue_role_arn=arn:aws:iam::963702399712:role/LabRole" \
            -var "script_s3_path=s3://third-glue-bkt-grp-three-nyc/scripts/latest/etl-glue-script.py" \
            -var "raw_data_s3_path=s3://raw-data-grp-3/cleaned-data/transformeddata/"

  run-etl:
    needs: infra-deploy
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Upload ETL Script to S3 with new folder
        run: |
          BUCKET_NAME="third-glue-bkt-grp-three-nyc"
          PREFIX="scripts/run"
          COUNT=1
          while aws s3 ls s3://$BUCKET_NAME/${PREFIX}${COUNT}/ > /dev/null 2>&1; do
            COUNT=$((COUNT+1))
          done
          FOLDER="${PREFIX}${COUNT}"
          aws s3 cp ${{ env.SCRIPT_PATH }} s3://$BUCKET_NAME/$FOLDER/etl-glue-script.py
          echo "Script uploaded to s3://$BUCKET_NAME/$FOLDER/"

      - name: Start Glue ETL Job
        id: start_etl
        run: |
          JOB_RUN_ID=$(aws glue start-job-run --job-name ${{ env.ETL_JOB_NAME }} --query 'JobRunId' --output text)
          echo "ETL JobRunId: $JOB_RUN_ID"
          echo "job_run_id=$JOB_RUN_ID" >> $GITHUB_OUTPUT

      - name: Wait for ETL Job Completion
        run: |
          JOB_RUN_ID=${{ steps.start_etl.outputs.job_run_id }}
          while true; do
            STATUS=$(aws glue get-job-run --job-name ${{ env.ETL_JOB_NAME }} --run-id "$JOB_RUN_ID" --query 'JobRun.JobRunState' --output text)
            echo "Current ETL job status: $STATUS"
            if [ "$STATUS" == "SUCCEEDED" ]; then
              echo "ETL Job completed successfully."
              break
            elif [ "$STATUS" == "FAILED" ] || [ "$STATUS" == "STOPPED" ]; then
              echo "ETL Job failed or stopped."
              exit 1
            fi
            sleep 30
          done

      - name: Get Terraform outputs
        working-directory: ./infra
        run: |
          CRAWLER_NAME=$(terraform output -raw crawler_name)
          echo "CRAWLER_NAME=$CRAWLER_NAME" >> $GITHUB_ENV




      - name: Start Glue Crawler
        run: aws glue start-crawler --name $CRAWLER_NAME
#